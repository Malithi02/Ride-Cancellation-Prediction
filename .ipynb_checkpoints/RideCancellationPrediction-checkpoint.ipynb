{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e802c1-8608-449f-b1c3-5f74f484d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy scikit-learn matplotlib seaborn imbalanced-learn xgboost joblib streamlit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff71f7-d546-49da-ab0c-132d59c97522",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem Definition\n",
    "#- **Problem:** High ride cancellation rates in NCR impact revenue and customer satisfaction.\n",
    "#- **Objective:** Build a machine learning model to predict cancellations and create a Streamlit dashboard for insights.\n",
    "#- **Dataset:** ncr_ride_bookings.csv (21 columns, ~148,770 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdaa49-2251-4a09-85df-14fe5a77841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Understanding\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/ncr_ride_bookings.csv')  # Replace with your file path\n",
    "\n",
    "# Basic inspection\n",
    "print(\"Dataset Shape:\", df.shape)  # Should be ~148770 rows, 21 columns\n",
    "print(\"\\nData Types:\\n\", df.dtypes)\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())  # High in Avg VTAT, Avg CTAT, etc., as per SOW\n",
    "print(\"\\nSample Data:\\n\", df.head(5))\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "print(\"\\nNumerical Summary:\\n\", df.describe())\n",
    "\n",
    "# Check class distribution for 'Booking Status' (imbalance check)\n",
    "print(\"\\nBooking Status Distribution:\\n\", df['Booking Status'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57f52c-8c90-4d26-bbc7-592ae5e414ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning and Preparation\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Handle missing values\n",
    "numerical_cols = ['Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance', 'Driver Ratings', 'Customer Rating']\n",
    "categorical_cols = ['Vehicle Type', 'Pickup Location', 'Drop Location', 'Payment Method', 'Reason for cancelling by Customer', 'Driver Cancellation Reason', 'Incomplete Rides Reason']\n",
    "\n",
    "# Impute numerical with median\n",
    "for col in numerical_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Impute categorical with 'Unknown'\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "# Handle flags (binary-like, fill with 0)\n",
    "flag_cols = ['Cancelled Rides by Customer', 'Cancelled Rides by Driver', 'Incomplete Rides']\n",
    "for col in flag_cols:\n",
    "    df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "# Convert Date and Time to datetime\n",
    "#df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "#df.drop(['Date', 'Time'], axis=1, inplace=True)  # Drop original\n",
    "# Fixed Datetime parsing (use '%Y-%m-%d %H:%M:%S' based on error; assumes Date is 'YYYY-MM-DD')\n",
    "df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%Y-%m-%d %H:%M:%S')\n",
    "df.drop(['Date', 'Time'], axis=1, inplace=True)\n",
    "\n",
    "# Outlier handling (IQR for numerical)\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[col] = np.clip(df[col], lower_bound, upper_bound)  # Cap outliers\n",
    "\n",
    "# Drop unnecessary IDs (not predictive)\n",
    "df.drop(['Booking ID', 'Customer ID'], axis=1, inplace=True)\n",
    "\n",
    "# Target transformation (binary: True for cancellation, False for Completed)\n",
    "df['Cancelled'] = df['Booking Status'].apply(lambda x: False if x == 'Completed' else True)\n",
    "df.drop('Booking Status', axis=1, inplace=True)\n",
    "\n",
    "# Split data (80% train, 10% val, 10% test)\n",
    "X = df.drop('Cancelled', axis=1)\n",
    "y = df['Cancelled']\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Scaling (after split, fit on train)\n",
    "scaler = StandardScaler()\n",
    "numerical_to_scale = ['Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance']  # Select continuous\n",
    "X_train[numerical_to_scale] = scaler.fit_transform(X_train[numerical_to_scale])\n",
    "X_val[numerical_to_scale] = scaler.transform(X_val[numerical_to_scale])\n",
    "X_test[numerical_to_scale] = scaler.transform(X_test[numerical_to_scale])\n",
    "\n",
    "print(\"Data Cleaned and Split. Train Shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d22c8b-34f1-4e2b-8708-f8c24f295671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Temporal features\n",
    "X_train['hour_of_day'] = X_train['Datetime'].dt.hour\n",
    "X_train['day_of_week'] = X_train['Datetime'].dt.dayofweek\n",
    "X_train['is_weekend'] = X_train['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "X_val['hour_of_day'] = X_val['Datetime'].dt.hour\n",
    "X_val['day_of_week'] = X_val['Datetime'].dt.dayofweek\n",
    "X_val['is_weekend'] = X_val['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "X_test['hour_of_day'] = X_test['Datetime'].dt.hour\n",
    "X_test['day_of_week'] = X_test['Datetime'].dt.dayofweek\n",
    "X_test['is_weekend'] = X_test['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Peak/Off-peak flag (e.g., peak: 7-10 AM, 5-8 PM)\n",
    "def is_peak(hour):\n",
    "    return 1 if (7 <= hour <= 10) or (17 <= hour <= 20) else 0\n",
    "X_train['peak_flag'] = X_train['hour_of_day'].apply(is_peak)\n",
    "X_val['peak_flag'] = X_val['hour_of_day'].apply(is_peak)\n",
    "X_test['peak_flag'] = X_test['hour_of_day'].apply(is_peak)\n",
    "\n",
    "# VTAT buckets (low <5, medium 5-10, high >10)\n",
    "def vtat_bucket(vtat):\n",
    "    if vtat < 5: return 'Low'\n",
    "    elif vtat <= 10: return 'Medium'\n",
    "    else: return 'High'\n",
    "X_train['vtat_bucket'] = X_train['Avg VTAT'].apply(vtat_bucket)\n",
    "X_val['vtat_bucket'] = X_val['Avg VTAT'].apply(vtat_bucket)\n",
    "X_test['vtat_bucket'] = X_test['Avg VTAT'].apply(vtat_bucket)\n",
    "\n",
    "# High/Low fare flag (above median = high)\n",
    "median_value = X_train['Booking Value'].median()\n",
    "X_train['high_fare_flag'] = X_train['Booking Value'].apply(lambda x: 1 if x > median_value else 0)\n",
    "X_val['high_fare_flag'] = X_val['Booking Value'].apply(lambda x: 1 if x > median_value else 0)\n",
    "X_test['high_fare_flag'] = X_test['Booking Value'].apply(lambda x: 1 if x > median_value else 0)\n",
    "\n",
    "# Reliability scores (customer cancellation rate - group by Customer ID, but since IDs dropped, approximate with flags)\n",
    "X_train['customer_reliability'] = 1 - X_train['Cancelled Rides by Customer']  # Simple: 1 if no past cancel, 0 if yes\n",
    "X_val['customer_reliability'] = 1 - X_val['Cancelled Rides by Customer']\n",
    "X_test['customer_reliability'] = 1 - X_test['Cancelled Rides by Customer']\n",
    "\n",
    "X_train['driver_reliability'] = 1 - X_train['Cancelled Rides by Driver']\n",
    "X_val['driver_reliability'] = 1 - X_val['Cancelled Rides by Driver']\n",
    "X_test['driver_reliability'] = 1 - X_test['Cancelled Rides by Driver']\n",
    "\n",
    "# Ride speed (avoid division by zero)\n",
    "X_train['ride_speed'] = X_train['Ride Distance'] / X_train['Avg CTAT'].replace(0, np.nan).fillna(1)\n",
    "X_val['ride_speed'] = X_val['Ride Distance'] / X_val['Avg CTAT'].replace(0, np.nan).fillna(1)\n",
    "X_test['ride_speed'] = X_test['Ride Distance'] / X_test['Avg CTAT'].replace(0, np.nan).fillna(1)\n",
    "\n",
    "# Drop Datetime after extraction\n",
    "X_train.drop('Datetime', axis=1, inplace=True)\n",
    "X_val.drop('Datetime', axis=1, inplace=True)\n",
    "X_test.drop('Datetime', axis=1, inplace=True)\n",
    "\n",
    "# Encoding\n",
    "ohe_cols = ['Vehicle Type', 'Payment Method', 'vtat_bucket']  # One-hot\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_train_ohe = pd.DataFrame(ohe.fit_transform(X_train[ohe_cols]), columns=ohe.get_feature_names_out(), index=X_train.index)\n",
    "X_val_ohe = pd.DataFrame(ohe.transform(X_val[ohe_cols]), columns=ohe.get_feature_names_out(), index=X_val.index)\n",
    "X_test_ohe = pd.DataFrame(ohe.transform(X_test[ohe_cols]), columns=ohe.get_feature_names_out(), index=X_test.index)\n",
    "\n",
    "# Label encoding for locations and reasons (high cardinality, so label)\n",
    "le_cols = ['Pickup Location', 'Drop Location', 'Reason for cancelling by Customer', 'Driver Cancellation Reason', 'Incomplete Rides Reason']\n",
    "le_dict = {}\n",
    "for col in le_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col])\n",
    "    X_val[col] = le.transform(X_val[col].map(lambda s: '<unknown>' if s not in le.classes_ else s))\n",
    "    X_test[col] = le.transform(X_test[col].map(lambda s: '<unknown>' if s not in le.classes_ else s))\n",
    "    le.classes_ = np.append(le.classes_, '<unknown>')  # Handle unknowns\n",
    "    le_dict[col] = le\n",
    "\n",
    "# Combine encoded and drop originals\n",
    "X_train = pd.concat([X_train.drop(ohe_cols, axis=1), X_train_ohe], axis=1)\n",
    "X_val = pd.concat([X_val.drop(ohe_cols, axis=1), X_val_ohe], axis=1)\n",
    "X_test = pd.concat([X_test.drop(ohe_cols, axis=1), X_test_ohe], axis=1)\n",
    "\n",
    "print(\"Features Engineered. Train Shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795ac39-d35c-4838-a9e9-e469b57dcb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis (EDA)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Full df for EDA (before split)\n",
    "df_eda = df.copy()  # Use original for visuals\n",
    "\n",
    "# Cancellation trends by Vehicle Type\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(data=df_eda, x='Vehicle Type', hue='Cancelled')\n",
    "plt.title('Cancellations by Vehicle Type')\n",
    "plt.savefig('vehicle_cancellations.png')  # Save for report\n",
    "plt.show()\n",
    "\n",
    "# By hour\n",
    "df_eda['hour'] = df_eda['Datetime'].dt.hour\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=df_eda, x='hour', hue='Cancelled', multiple='stack')\n",
    "plt.title('Cancellations by Hour')\n",
    "plt.savefig('hour_cancellations.png')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap (numerical only)\n",
    "num_df = df_eda.select_dtypes(include=np.number)\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(num_df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.savefig('correlation_heatmap.png')\n",
    "plt.show()\n",
    "\n",
    "# Cancellation reasons (top 5)\n",
    "top_reasons = df_eda['Reason for cancelling by Customer'].value_counts().head(5)\n",
    "plt.figure(figsize=(8,5))\n",
    "top_reasons.plot(kind='bar')\n",
    "plt.title('Top Customer Cancellation Reasons')\n",
    "plt.savefig('reasons.png')\n",
    "plt.show()\n",
    "\n",
    "# Print insights\n",
    "print(\"Key Insights: High cancellations in peak hours, certain vehicles, low ratings correlate with cancels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18620bb-87e1-489e-ac3b-43884519a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Development\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Handle imbalance with SMOTE (on train only)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'Neural Network': MLPClassifier(max_iter=500)\n",
    "}\n",
    "\n",
    "# Hyperparam tuning (example for RF and XGBoost)\n",
    "param_grids = {\n",
    "    'Random Forest': {'n_estimators': [50, 100], 'max_depth': [10, 20]},\n",
    "    'XGBoost': {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1]}\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "for name, model in models.items():\n",
    "    if name in param_grids:\n",
    "        grid = GridSearchCV(model, param_grids[name], cv=5, scoring='f1')\n",
    "        grid.fit(X_train_res, y_train_res)\n",
    "        best_models[name] = grid.best_estimator_\n",
    "        print(f\"{name} Best Params: {grid.best_params_}\")\n",
    "    else:\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        best_models[name] = model\n",
    "\n",
    "print(\"Models Trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794298e0-89d1-4fad-8142-072552f252ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb4d36-f2c0-4f87-8134-25266db124d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
