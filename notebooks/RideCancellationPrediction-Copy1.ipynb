{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4e802c1-8608-449f-b1c3-5f74f484d654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in c:\\programdata\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\programdata\\anaconda3\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\amanda\\appdata\\roaming\\python\\python313\\site-packages (3.0.5)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: streamlit in c:\\programdata\\anaconda3\\lib\\site-packages (1.45.1)\n",
      "Requirement already satisfied: pyngrok in c:\\users\\amanda\\appdata\\roaming\\python\\python313\\site-packages (7.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (5.5.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (19.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (4.0.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.31.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyngrok) (6.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scikit-learn matplotlib seaborn imbalanced-learn xgboost joblib streamlit pyngrok\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f689574-ffef-49da-a6cc-9d6381dd30d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%writefile` not found.\n"
     ]
    }
   ],
   "source": [
    "# Create app.py\n",
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Load saved models and preprocessors with correct paths\n",
    "model_path = 'best_model.pkl'\n",
    "scaler_path = 'scaler.pkl' \n",
    "ohe_path = 'ohe.pkl'\n",
    "le_dict_path = 'le_dict.pkl'\n",
    "\n",
    "# Check if files exist in models folder first, then root directory\n",
    "if os.path.exists(f'models/{model_path}'):\n",
    "    best_model = joblib.load(f'models/{model_path}')\n",
    "    scaler = joblib.load(f'models/{scaler_path}')\n",
    "    ohe = joblib.load(f'models/{ohe_path}')\n",
    "    le_dict = joblib.load(f'models/{le_dict_path}')\n",
    "    st.success(\"Models and preprocessors loaded successfully from models/ directory!\")\n",
    "elif os.path.exists(model_path):\n",
    "    best_model = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    ohe = joblib.load(ohe_path)\n",
    "    le_dict = joblib.load(le_dict_path)\n",
    "    st.success(\"Models and preprocessors loaded successfully from root directory!\")\n",
    "else:\n",
    "    st.error(f\"Model files not found. Looking for: {model_path}, {scaler_path}, {ohe_path}, {le_dict_path}\")\n",
    "    st.error(\"Please ensure all .pkl files are available.\")\n",
    "    st.stop()\n",
    "\n",
    "# Set page config\n",
    "st.set_page_config(page_title=\"Ride Cancellation Predictor\", layout=\"wide\")\n",
    "st.title(\"Ride Cancellation Prediction Dashboard\")\n",
    "\n",
    "# Sidebar navigation\n",
    "page = st.sidebar.selectbox(\"Section\", [\"Predict\", \"Dashboard\", \"Insights\"])\n",
    "\n",
    "# Prediction Page\n",
    "if page == \"Predict\":\n",
    "    st.header(\"Real-Time Prediction\")\n",
    "    with st.form(key='prediction_form'):\n",
    "        vehicle_type = st.selectbox(\"Vehicle Type\", ['Go Mini', 'Go Sedan', 'Auto'])\n",
    "        avg_vtat = st.number_input(\"Average VTAT (minutes)\", min_value=0.0, value=5.0)\n",
    "        avg_ctat = st.number_input(\"Average CTAT (minutes)\", min_value=0.0, value=10.0)\n",
    "        booking_value = st.number_input(\"Booking Value\", min_value=0.0, value=200.0)\n",
    "        ride_distance = st.number_input(\"Ride Distance (km)\", min_value=0.0, value=5.0)\n",
    "        driver_rating = st.number_input(\"Driver Rating\", min_value=0.0, max_value=5.0, value=4.5)\n",
    "        customer_rating = st.number_input(\"Customer Rating\", min_value=0.0, max_value=5.0, value=4.5)\n",
    "        hour_of_day = st.slider(\"Hour of Day\", 0, 23, 12)\n",
    "        submit_button = st.form_submit_button(label='Predict')\n",
    "\n",
    "        if submit_button:\n",
    "            try:\n",
    "                # Prepare input data\n",
    "                input_data = pd.DataFrame({\n",
    "                    'Avg VTAT': [avg_vtat],\n",
    "                    'Avg CTAT': [avg_ctat],\n",
    "                    'Booking Value': [booking_value],\n",
    "                    'Ride Distance': [ride_distance],\n",
    "                    'Driver Ratings': [driver_rating],\n",
    "                    'Customer Rating': [customer_rating],\n",
    "                    'hour_of_day': [hour_of_day],\n",
    "                    'Vehicle Type': [vehicle_type]\n",
    "                })\n",
    "\n",
    "                # Scale numerical features\n",
    "                numerical_cols = ['Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance']\n",
    "                input_data[numerical_cols] = scaler.transform(input_data[numerical_cols])\n",
    "\n",
    "                # One-hot encode categorical features\n",
    "                ohe_cols = ['Vehicle Type']\n",
    "                ohe_input = pd.DataFrame(ohe.transform(input_data[ohe_cols]), \n",
    "                                       columns=ohe.get_feature_names_out(ohe_cols))\n",
    "                input_data = pd.concat([input_data.drop(ohe_cols, axis=1), ohe_input], axis=1)\n",
    "\n",
    "                # Predict\n",
    "                prediction = best_model.predict(input_data)[0]\n",
    "                probability = best_model.predict_proba(input_data)[0][1]\n",
    "                st.success(f\"Prediction: {'High Risk (Cancelled)' if prediction == 1 else 'Low Risk (Completed)'}\")\n",
    "                st.info(f\"Cancellation Probability: {probability:.2%}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                st.error(f\"Prediction error: {str(e)}\")\n",
    "\n",
    "# Dashboard Page\n",
    "elif page == \"Dashboard\":\n",
    "    st.header(\"Visual Insights\")\n",
    "    \n",
    "    # Check if image files exist\n",
    "    image_files = {\n",
    "        \"Vehicle Cancellations\": \"vehicle_cancellations.png\",\n",
    "        \"Hourly Cancellations\": \"hour_cancellations.png\", \n",
    "        \"Correlation Heatmap\": \"correlation_heatmap.png\"\n",
    "    }\n",
    "    \n",
    "    for title, filename in image_files.items():\n",
    "        if os.path.exists(filename):\n",
    "            st.image(filename, caption=title)\n",
    "        else:\n",
    "            st.warning(f\"Image not found: {filename}\")\n",
    "    \n",
    "    # Batch prediction option\n",
    "    uploaded_file = st.file_uploader(\"Upload CSV for Batch Prediction\", type=\"csv\")\n",
    "    if uploaded_file:\n",
    "        try:\n",
    "            df = pd.read_csv(uploaded_file)\n",
    "            st.write(\"Uploaded Data Preview:\", df.head())\n",
    "            st.success(f\"Successfully loaded {len(df)} records\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error reading CSV: {str(e)}\")\n",
    "\n",
    "# Insights Page\n",
    "elif page == \"Insights\":\n",
    "    st.header(\"Insights & Recommendations\")\n",
    "    st.write(\"\"\"\n",
    "    - **Key Insight:** High cancellations occur during peak hours (7-10 AM, 5-8 PM), likely due to increased VTAT.\n",
    "    - **Recommendation:** Increase driver availability during these times and offer incentives to reduce wait times.\n",
    "    - **Additional Insight:** Vehicle Type 'Auto' shows higher cancellation rates, possibly due to reliability issues.\n",
    "    - **Recommendation:** Enhance maintenance or driver training for 'Auto' vehicles.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab241e62-f105-4d01-afea-d30b809a8a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngrok version 3.30.0\n"
     ]
    }
   ],
   "source": [
    "!ngrok --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b57cfc29-6eca-49ba-b44c-4676a517677d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Streamlit with app: C:\\Users\\Amanda\\RideCancellationPredictionModel\\app\\app.py\n",
      "Streamlit errors: 2025-10-02 13:22:37.248 \n",
      "\n",
      "Streamlit app is live at: NgrokTunnel: \"https://unplastic-irrepealably-leslie.ngrok-free.dev\" -> \"http://127.0.0.1:8501\"\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Set your ngrok authtoken (get from ngrok.com dashboard)\n",
    "ngrok.set_auth_token(\"33CHnXAsJjuuB47XjkD3l9qvxDY_Lou7hMq9APAQRKP3Ji6B\")\n",
    "\n",
    "# Resolve project root (parent of notebooks/) and app path\n",
    "project_root = Path.cwd().parent\n",
    "app_path = project_root / 'app' / 'app.py'\n",
    "print('Launching Streamlit with app:', app_path)\n",
    "\n",
    "# Start Streamlit in the background from project root so relative paths work\n",
    "process = subprocess.Popen(\n",
    "    ['streamlit', 'run', str(app_path)],\n",
    "    cwd=str(project_root),\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Give Streamlit time to boot\n",
    "time.sleep(5)\n",
    "\n",
    "# Non-blocking check for immediate stderr without killing the process\n",
    "if process.poll() is not None:\n",
    "    err = process.stderr.read()\n",
    "    if err:\n",
    "        print('Streamlit errors:', err)\n",
    "\n",
    "# Create ngrok tunnel, forcing IPv4 to avoid [::1] issues\n",
    "public_url = ngrok.connect(addr=\"127.0.0.1:8501\", proto=\"http\")\n",
    "print(f\"Streamlit app is live at: {public_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17ff71f7-d546-49da-ab0c-132d59c97522",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem Definition\n",
    "#- **Problem:** High ride cancellation rates in NCR impact revenue and customer satisfaction.\n",
    "#- **Objective:** Build a machine learning model to predict cancellations and create a Streamlit dashboard for insights.\n",
    "#- **Dataset:** ncr_ride_bookings.csv (21 columns, ~148,770 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebbdaa49-2251-4a09-85df-14fe5a77841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (150000, 21)\n",
      "\n",
      "Data Types:\n",
      " Date                                  object\n",
      "Time                                  object\n",
      "Booking ID                            object\n",
      "Booking Status                        object\n",
      "Customer ID                           object\n",
      "Vehicle Type                          object\n",
      "Pickup Location                       object\n",
      "Drop Location                         object\n",
      "Avg VTAT                             float64\n",
      "Avg CTAT                             float64\n",
      "Cancelled Rides by Customer          float64\n",
      "Reason for cancelling by Customer     object\n",
      "Cancelled Rides by Driver            float64\n",
      "Driver Cancellation Reason            object\n",
      "Incomplete Rides                     float64\n",
      "Incomplete Rides Reason               object\n",
      "Booking Value                        float64\n",
      "Ride Distance                        float64\n",
      "Driver Ratings                       float64\n",
      "Customer Rating                      float64\n",
      "Payment Method                        object\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      " Date                                      0\n",
      "Time                                      0\n",
      "Booking ID                                0\n",
      "Booking Status                            0\n",
      "Customer ID                               0\n",
      "Vehicle Type                              0\n",
      "Pickup Location                           0\n",
      "Drop Location                             0\n",
      "Avg VTAT                              10500\n",
      "Avg CTAT                              48000\n",
      "Cancelled Rides by Customer          139500\n",
      "Reason for cancelling by Customer    139500\n",
      "Cancelled Rides by Driver            123000\n",
      "Driver Cancellation Reason           123000\n",
      "Incomplete Rides                     141000\n",
      "Incomplete Rides Reason              141000\n",
      "Booking Value                         48000\n",
      "Ride Distance                         48000\n",
      "Driver Ratings                        57000\n",
      "Customer Rating                       57000\n",
      "Payment Method                        48000\n",
      "dtype: int64\n",
      "\n",
      "Sample Data:\n",
      "          Date      Time    Booking ID   Booking Status   Customer ID  \\\n",
      "0  2024-03-23  12:29:38  \"CNR5884300\"  No Driver Found  \"CID1982111\"   \n",
      "1  2024-11-29  18:01:39  \"CNR1326809\"       Incomplete  \"CID4604802\"   \n",
      "2  2024-08-23  08:56:10  \"CNR8494506\"        Completed  \"CID9202816\"   \n",
      "3  2024-10-21  17:17:25  \"CNR8906825\"        Completed  \"CID2610914\"   \n",
      "4  2024-09-16  22:08:00  \"CNR1950162\"        Completed  \"CID9933542\"   \n",
      "\n",
      "    Vehicle Type      Pickup Location      Drop Location  Avg VTAT  Avg CTAT  \\\n",
      "0          eBike          Palam Vihar            Jhilmil       NaN       NaN   \n",
      "1       Go Sedan        Shastri Nagar  Gurgaon Sector 56       4.9      14.0   \n",
      "2           Auto              Khandsa      Malviya Nagar      13.4      25.8   \n",
      "3  Premier Sedan  Central Secretariat           Inderlok      13.1      28.5   \n",
      "4           Bike     Ghitorni Village        Khan Market       5.3      19.6   \n",
      "\n",
      "   ...  Reason for cancelling by Customer Cancelled Rides by Driver  \\\n",
      "0  ...                                NaN                       NaN   \n",
      "1  ...                                NaN                       NaN   \n",
      "2  ...                                NaN                       NaN   \n",
      "3  ...                                NaN                       NaN   \n",
      "4  ...                                NaN                       NaN   \n",
      "\n",
      "   Driver Cancellation Reason Incomplete Rides  Incomplete Rides Reason  \\\n",
      "0                         NaN              NaN                      NaN   \n",
      "1                         NaN              1.0        Vehicle Breakdown   \n",
      "2                         NaN              NaN                      NaN   \n",
      "3                         NaN              NaN                      NaN   \n",
      "4                         NaN              NaN                      NaN   \n",
      "\n",
      "  Booking Value  Ride Distance  Driver Ratings  Customer Rating  \\\n",
      "0           NaN            NaN             NaN              NaN   \n",
      "1         237.0           5.73             NaN              NaN   \n",
      "2         627.0          13.58             4.9              4.9   \n",
      "3         416.0          34.02             4.6              5.0   \n",
      "4         737.0          48.21             4.1              4.3   \n",
      "\n",
      "   Payment Method  \n",
      "0             NaN  \n",
      "1             UPI  \n",
      "2      Debit Card  \n",
      "3             UPI  \n",
      "4             UPI  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Numerical Summary:\n",
      "             Avg VTAT       Avg CTAT  Cancelled Rides by Customer  \\\n",
      "count  139500.000000  102000.000000                      10500.0   \n",
      "mean        8.456352      29.149636                          1.0   \n",
      "std         3.773564       8.902577                          0.0   \n",
      "min         2.000000      10.000000                          1.0   \n",
      "25%         5.300000      21.600000                          1.0   \n",
      "50%         8.300000      28.800000                          1.0   \n",
      "75%        11.300000      36.800000                          1.0   \n",
      "max        20.000000      45.000000                          1.0   \n",
      "\n",
      "       Cancelled Rides by Driver  Incomplete Rides  Booking Value  \\\n",
      "count                    27000.0            9000.0  102000.000000   \n",
      "mean                         1.0               1.0     508.295912   \n",
      "std                          0.0               0.0     395.805774   \n",
      "min                          1.0               1.0      50.000000   \n",
      "25%                          1.0               1.0     234.000000   \n",
      "50%                          1.0               1.0     414.000000   \n",
      "75%                          1.0               1.0     689.000000   \n",
      "max                          1.0               1.0    4277.000000   \n",
      "\n",
      "       Ride Distance  Driver Ratings  Customer Rating  \n",
      "count  102000.000000    93000.000000     93000.000000  \n",
      "mean       24.637012        4.230992         4.404584  \n",
      "std        14.002138        0.436871         0.437819  \n",
      "min         1.000000        3.000000         3.000000  \n",
      "25%        12.460000        4.100000         4.200000  \n",
      "50%        23.720000        4.300000         4.500000  \n",
      "75%        36.820000        4.600000         4.800000  \n",
      "max        50.000000        5.000000         5.000000  \n",
      "\n",
      "Booking Status Distribution:\n",
      " Booking Status\n",
      "Completed                0.62\n",
      "Cancelled by Driver      0.18\n",
      "No Driver Found          0.07\n",
      "Cancelled by Customer    0.07\n",
      "Incomplete               0.06\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Data Understanding\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/ncr_ride_bookings.csv')  # Replace with your file path\n",
    "\n",
    "# Basic inspection\n",
    "print(\"Dataset Shape:\", df.shape)  # Should be ~148770 rows, 21 columns\n",
    "print(\"\\nData Types:\\n\", df.dtypes)\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())  # High in Avg VTAT, Avg CTAT, etc., as per SOW\n",
    "print(\"\\nSample Data:\\n\", df.head(5))\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "print(\"\\nNumerical Summary:\\n\", df.describe())\n",
    "\n",
    "# Check class distribution for 'Booking Status' (imbalance check)\n",
    "print(\"\\nBooking Status Distribution:\\n\", df['Booking Status'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a57f52c-8c90-4d26-bbc7-592ae5e414ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaned and Split. Train Shape: (120000, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/scaler.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Cleaning and Preparation\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Handle missing values\n",
    "numerical_cols = ['Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance', 'Driver Ratings', 'Customer Rating']\n",
    "categorical_cols = ['Vehicle Type', 'Pickup Location', 'Drop Location', 'Payment Method', 'Reason for cancelling by Customer', 'Driver Cancellation Reason', 'Incomplete Rides Reason']\n",
    "\n",
    "# Impute numerical with median\n",
    "for col in numerical_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Impute categorical with 'Unknown'\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "# Handle flags (binary-like, fill with 0)\n",
    "flag_cols = ['Cancelled Rides by Customer', 'Cancelled Rides by Driver', 'Incomplete Rides']\n",
    "for col in flag_cols:\n",
    "    df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "# Convert Date and Time to datetime\n",
    "#df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "#df.drop(['Date', 'Time'], axis=1, inplace=True)  # Drop original\n",
    "# Fixed Datetime parsing (use '%Y-%m-%d %H:%M:%S' based on error; assumes Date is 'YYYY-MM-DD')\n",
    "df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%Y-%m-%d %H:%M:%S')\n",
    "df.drop(['Date', 'Time'], axis=1, inplace=True)\n",
    "\n",
    "# Outlier handling (IQR for numerical)\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[col] = np.clip(df[col], lower_bound, upper_bound)  # Cap outliers\n",
    "\n",
    "# Drop unnecessary IDs (not predictive)\n",
    "df.drop(['Booking ID', 'Customer ID'], axis=1, inplace=True)\n",
    "\n",
    "# Target transformation (binary: True for cancellation, False for Completed)\n",
    "df['Cancelled'] = df['Booking Status'].apply(lambda x: False if x == 'Completed' else True)\n",
    "df.drop('Booking Status', axis=1, inplace=True)\n",
    "\n",
    "# Split data (80% train, 10% val, 10% test)\n",
    "X = df.drop('Cancelled', axis=1)\n",
    "y = df['Cancelled']\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Preserve raw VTAT for bucketing, then scale numericals (fit on train)\n",
    "X_train['Avg VTAT Raw'] = X_train['Avg VTAT']\n",
    "X_val['Avg VTAT Raw'] = X_val['Avg VTAT']\n",
    "X_test['Avg VTAT Raw'] = X_test['Avg VTAT']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_to_scale = ['Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance']  # Select continuous\n",
    "X_train[numerical_to_scale] = scaler.fit_transform(X_train[numerical_to_scale])\n",
    "X_val[numerical_to_scale] = scaler.transform(X_val[numerical_to_scale])\n",
    "X_test[numerical_to_scale] = scaler.transform(X_test[numerical_to_scale])\n",
    "\n",
    "print(\"Data Cleaned and Split. Train Shape:\", X_train.shape)\n",
    "import joblib\n",
    "joblib.dump(scaler, '../models/scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0d22c8b-34f1-4e2b-8708-f8c24f295671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Engineered. Train Shape: (120000, 38)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/ohe.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Engineering\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Temporal features\n",
    "X_train['hour_of_day'] = X_train['Datetime'].dt.hour\n",
    "X_train['day_of_week'] = X_train['Datetime'].dt.dayofweek\n",
    "X_train['is_weekend'] = X_train['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "X_val['hour_of_day'] = X_val['Datetime'].dt.hour\n",
    "X_val['day_of_week'] = X_val['Datetime'].dt.dayofweek\n",
    "X_val['is_weekend'] = X_val['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "X_test['hour_of_day'] = X_test['Datetime'].dt.hour\n",
    "X_test['day_of_week'] = X_test['Datetime'].dt.dayofweek\n",
    "X_test['is_weekend'] = X_test['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Peak/Off-peak flag (e.g., peak: 7-10 AM, 5-8 PM)\n",
    "def is_peak(hour):\n",
    "    return 1 if (7 <= hour <= 10) or (17 <= hour <= 20) else 0\n",
    "X_train['peak_flag'] = X_train['hour_of_day'].apply(is_peak)\n",
    "X_val['peak_flag'] = X_val['hour_of_day'].apply(is_peak)\n",
    "X_test['peak_flag'] = X_test['hour_of_day'].apply(is_peak)\n",
    "\n",
    "# VTAT buckets (low <5, medium 5-10, high >10) using raw VTAT prior to scaling\n",
    "def vtat_bucket(vtat):\n",
    "    if vtat < 5: return 'Low'\n",
    "    elif vtat <= 10: return 'Medium'\n",
    "    else: return 'High'\n",
    "X_train['vtat_bucket'] = X_train['Avg VTAT Raw'].apply(vtat_bucket)\n",
    "X_val['vtat_bucket'] = X_val['Avg VTAT Raw'].apply(vtat_bucket)\n",
    "X_test['vtat_bucket'] = X_test['Avg VTAT Raw'].apply(vtat_bucket)\n",
    "\n",
    "# High/Low fare flag (above median = high)\n",
    "median_value = X_train['Booking Value'].median()\n",
    "X_train['high_fare_flag'] = X_train['Booking Value'].apply(lambda x: 1 if x > median_value else 0)\n",
    "X_val['high_fare_flag'] = X_val['Booking Value'].apply(lambda x: 1 if x > median_value else 0)\n",
    "X_test['high_fare_flag'] = X_test['Booking Value'].apply(lambda x: 1 if x > median_value else 0)\n",
    "\n",
    "# Reliability scores (customer cancellation rate - group by Customer ID, but since IDs dropped, approximate with flags)\n",
    "X_train['customer_reliability'] = 1 - X_train['Cancelled Rides by Customer']  # Simple: 1 if no past cancel, 0 if yes\n",
    "X_val['customer_reliability'] = 1 - X_val['Cancelled Rides by Customer']\n",
    "X_test['customer_reliability'] = 1 - X_test['Cancelled Rides by Customer']\n",
    "\n",
    "X_train['driver_reliability'] = 1 - X_train['Cancelled Rides by Driver']\n",
    "X_val['driver_reliability'] = 1 - X_val['Cancelled Rides by Driver']\n",
    "X_test['driver_reliability'] = 1 - X_test['Cancelled Rides by Driver']\n",
    "\n",
    "# Ride speed (avoid division by zero)\n",
    "X_train['ride_speed'] = X_train['Ride Distance'] / X_train['Avg CTAT'].replace(0, np.nan).fillna(1)\n",
    "X_val['ride_speed'] = X_val['Ride Distance'] / X_val['Avg CTAT'].replace(0, np.nan).fillna(1)\n",
    "X_test['ride_speed'] = X_test['Ride Distance'] / X_test['Avg CTAT'].replace(0, np.nan).fillna(1)\n",
    "\n",
    "# Drop Datetime after extraction\n",
    "X_train.drop('Datetime', axis=1, inplace=True)\n",
    "X_val.drop('Datetime', axis=1, inplace=True)\n",
    "X_test.drop('Datetime', axis=1, inplace=True)\n",
    "\n",
    "# Encoding\n",
    "ohe_cols = ['Vehicle Type', 'Payment Method', 'vtat_bucket']  # One-hot\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_train_ohe = pd.DataFrame(ohe.fit_transform(X_train[ohe_cols]), columns=ohe.get_feature_names_out(), index=X_train.index)\n",
    "X_val_ohe = pd.DataFrame(ohe.transform(X_val[ohe_cols]), columns=ohe.get_feature_names_out(), index=X_val.index)\n",
    "X_test_ohe = pd.DataFrame(ohe.transform(X_test[ohe_cols]), columns=ohe.get_feature_names_out(), index=X_test.index)\n",
    "\n",
    "# Label encoding for locations and reasons (high cardinality, so label)\n",
    "le_cols = ['Pickup Location', 'Drop Location', 'Reason for cancelling by Customer', 'Driver Cancellation Reason', 'Incomplete Rides Reason']\n",
    "le_dict = {}\n",
    "for col in le_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col])\n",
    "    # Ensure '<unknown>' is in classes before transforming val/test\n",
    "    import numpy as np\n",
    "    if '<unknown>' not in le.classes_:\n",
    "        le.classes_ = np.unique(np.concatenate([le.classes_, ['<unknown>']]))\n",
    "    X_val[col] = le.transform(X_val[col].map(lambda s: s if s in le.classes_ else '<unknown>'))\n",
    "    X_test[col] = le.transform(X_test[col].map(lambda s: s if s in le.classes_ else '<unknown>'))\n",
    "    le_dict[col] = le\n",
    "\n",
    "# Combine encoded and drop originals\n",
    "X_train = pd.concat([X_train.drop(ohe_cols, axis=1), X_train_ohe], axis=1)\n",
    "X_val = pd.concat([X_val.drop(ohe_cols, axis=1), X_val_ohe], axis=1)\n",
    "X_test = pd.concat([X_test.drop(ohe_cols, axis=1), X_test_ohe], axis=1)\n",
    "\n",
    "# Optionally drop the raw helper column after usage\n",
    "X_train.drop(['Avg VTAT Raw'], axis=1, inplace=True)\n",
    "X_val.drop(['Avg VTAT Raw'], axis=1, inplace=True)\n",
    "X_test.drop(['Avg VTAT Raw'], axis=1, inplace=True)\n",
    "\n",
    "print(\"Features Engineered. Train Shape:\", X_train.shape)\n",
    "import joblib\n",
    "joblib.dump(le_dict, '../models/le_dict.pkl')\n",
    "joblib.dump(ohe, '../models/ohe.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795ac39-d35c-4838-a9e9-e469b57dcb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis (EDA)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Full df for EDA (before split)\n",
    "df_eda = df.copy()  # Use original for visuals\n",
    "\n",
    "# Cancellation trends by Vehicle Type\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(data=df_eda, x='Vehicle Type', hue='Cancelled')\n",
    "plt.title('Cancellations by Vehicle Type')\n",
    "plt.savefig('vehicle_cancellations.png')  # Save for report\n",
    "plt.show()\n",
    "\n",
    "# By hour\n",
    "df_eda['hour'] = df_eda['Datetime'].dt.hour\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=df_eda, x='hour', hue='Cancelled', multiple='stack')\n",
    "plt.title('Cancellations by Hour')\n",
    "plt.savefig('hour_cancellations.png')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap (numerical only)\n",
    "num_df = df_eda.select_dtypes(include=np.number)\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(num_df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.savefig('correlation_heatmap.png')\n",
    "plt.show()\n",
    "\n",
    "# Cancellation reasons (top 5)\n",
    "top_reasons = df_eda['Reason for cancelling by Customer'].value_counts().head(5)\n",
    "plt.figure(figsize=(8,5))\n",
    "top_reasons.plot(kind='bar')\n",
    "plt.title('Top Customer Cancellation Reasons')\n",
    "plt.savefig('reasons.png')\n",
    "plt.show()\n",
    "\n",
    "# Print insights\n",
    "print(\"Key Insights: High cancellations in peak hours, certain vehicles, low ratings correlate with cancels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18620bb-87e1-489e-ac3b-43884519a6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794298e0-89d1-4fad-8142-072552f252ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Development\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Handle imbalance with SMOTE (on train only)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Models with scaling where needed\n",
    "models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=2000, solver='lbfgs'))\n",
    "    ]),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    'Neural Network': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', MLPClassifier(max_iter=1000, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Hyperparameter grids (only for RF and XGBoost here)\n",
    "param_grids = {\n",
    "    'Random Forest': {'n_estimators': [50, 100], 'max_depth': [10, 20]},\n",
    "    'XGBoost': {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1]}\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "for name, model in models.items():\n",
    "    if name in param_grids:\n",
    "        grid = GridSearchCV(model, param_grids[name], cv=5, scoring='f1', n_jobs=-1)\n",
    "        grid.fit(X_train_res, y_train_res)\n",
    "        best_models[name] = grid.best_estimator_\n",
    "        print(f\"{name} Best Params: {grid.best_params_}\")\n",
    "    else:\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        best_models[name] = model\n",
    "\n",
    "print(\"âœ… All models trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb4d36-f2c0-4f87-8134-25266db124d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evalution\n",
    "# Evaluate on validation\n",
    "results = {}\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_val)\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(y_val, y_pred),\n",
    "        'Precision': precision_score(y_val, y_pred),\n",
    "        'Recall': recall_score(y_val, y_pred),\n",
    "        'F1': f1_score(y_val, y_pred)\n",
    "    }\n",
    "\n",
    "# Display in table\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Evaluation Results:\\n\", results_df)\n",
    "\n",
    "# Select best (e.g., highest F1)\n",
    "best_model_name = results_df['F1'].idxmax()\n",
    "best_model = best_models[best_model_name]\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "\n",
    "# Final test evaluation\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "print(f\"Test F1: {test_f1}\")\n",
    "import joblib\n",
    "joblib.dump(best_model, '../models/best_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0100ec-4a58-4c21-b52f-c0ad3bf1e7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
